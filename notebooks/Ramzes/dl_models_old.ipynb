{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zip = zipfile.ZipFile('data.zip', 'r') # open archive with archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_array(json):\n",
    "    \n",
    "    boxes = pd.json_normalize(json['data_result']['boxes'])[[\n",
    "        'mass', 'size.width', 'size.height', 'size.length'\n",
    "        ]]\n",
    "    target = json['data_result']['cargo_space']['calculation_info']['density_percent']\n",
    "    \n",
    "    return boxes.to_numpy().flatten(), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-09 17:52:54.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m33377.json length is more than 512. Skip.\u001b[0m\n",
      "\u001b[32m2024-02-09 17:52:54.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m34766.json length is more than 512. Skip.\u001b[0m\n",
      "\u001b[32m2024-02-09 17:52:54.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m34944.json length is more than 512. Skip.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "targets = []\n",
    "\n",
    "for data_info in data_zip.filelist: # for all archives inside\n",
    "    jsons_zip = zipfile.ZipFile(f'{data_info.filename}', 'r') # open archive with jsons\n",
    "    for jsons_zip_info in jsons_zip.filelist: \n",
    "        with jsons_zip.open(jsons_zip_info.filename) as file: # open json\n",
    "            json_file = json.loads(file.read())\n",
    "            try:\n",
    "                boxes, target = json_to_array(json_file)\n",
    "                if len(boxes) <= 512:\n",
    "                    boxes = np.array2string(\n",
    "                        boxes, separator=' ', formatter={'float_kind': lambda x: str(int(x))}\n",
    "                    ).replace('[', '').replace(']', '').replace('\\n', '')\n",
    "                    arr.append(str(boxes))\n",
    "                    targets.append(round(target))\n",
    "                else:\n",
    "                    logger.info(f'{jsons_zip_info.filename} length is more than 512. Skip.')\n",
    "                    pass\n",
    "            except Exception as e:\n",
    "                logger.warning(f'\\nError {e} \\n In file {jsons_zip_info.filename}')\n",
    "                pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, arr, targets, tokenizer: AutoTokenizer) -> None:\n",
    "        super(Dataset,self).__init__()\n",
    "        \n",
    "        self.arr = arr\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.labels = set(targets)\n",
    "\n",
    "        self.id2label = {idx:label for idx, label in enumerate(self.labels)}\n",
    "        self.label2id = {label:idx for idx, label in enumerate(self.labels)}\n",
    "        \n",
    "        self.encodings = tokenizer(arr,\n",
    "                                   padding=\"max_length\",\n",
    "                                   truncation=True,\n",
    "                                   max_length=512,\n",
    "                                   return_tensors=\"pt\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        item = {\n",
    "            'input_ids' : self.encodings['input_ids'][idx],\n",
    "            'attention_masks' : self.encodings['attention_mask'][idx],\n",
    "            'targets' : self.targets[idx]\n",
    "        }\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr, val_arr, train_labels, val_labels = train_test_split(arr, targets, train_size=0.8, random_state=42) # not enough data for stratifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & dataloader\n",
    "train_dataset = SequenceDataset(train_arr, train_labels, tokenizer)\n",
    "val_dataset = SequenceDataset(val_arr, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\",\n",
    "                                                           id2label=train_dataset.id2label,\n",
    "                                                           label2id=train_dataset.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=54, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "inp = batch['input_ids']\n",
    "msk = batch['attention_masks']\n",
    "targets = batch['targets']\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inp, msk).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.1324e-02,  1.5877e-01,  1.8534e-01, -2.2440e-01, -1.0671e-01,\n",
       "         -7.3002e-01, -8.6762e-02, -2.6212e-01,  3.8471e-02,  9.5836e-02,\n",
       "         -5.1918e-02, -5.5732e-02, -1.3814e-01, -4.2400e-01,  2.8129e-01,\n",
       "         -5.9642e-01,  1.9677e-02,  9.6603e-02, -3.1720e-01, -3.1919e-01,\n",
       "         -2.2364e-01, -1.8494e-01, -4.9326e-02,  8.6304e-02,  1.6323e-01,\n",
       "          6.5074e-02,  3.1736e-02, -8.1918e-02, -2.3737e-01, -7.5522e-02,\n",
       "         -9.2556e-02, -6.7558e-02,  1.3388e-01,  5.1918e-01, -1.6889e-01,\n",
       "         -1.4325e-01,  2.4757e-03,  3.2576e-01, -7.1208e-01, -1.7321e-01,\n",
       "         -1.4190e-01, -2.4352e-01,  4.9416e-01,  1.1689e-01,  3.7894e-01,\n",
       "          1.4997e-02, -1.1204e-01, -1.3797e-01, -1.4962e-01,  1.5934e-01,\n",
       "         -1.6081e-01, -2.1723e-01, -1.4116e-01,  1.8200e-01],\n",
       "        [-3.9210e-02,  1.9848e-01,  1.9677e-01, -3.0167e-01, -4.3886e-02,\n",
       "         -7.3666e-01, -2.5710e-02, -2.2825e-01,  1.1192e-02,  1.3202e-01,\n",
       "         -2.9506e-02,  4.2047e-02, -1.1006e-01, -4.5513e-01,  2.7098e-01,\n",
       "         -6.0627e-01,  9.8566e-03,  9.1895e-02, -3.2461e-01, -3.4797e-01,\n",
       "         -1.8563e-01, -2.0056e-01, -5.5475e-02,  6.1006e-02,  1.7923e-01,\n",
       "          7.7768e-02,  1.7737e-02, -1.0626e-01, -2.3119e-01, -6.6625e-02,\n",
       "         -4.7677e-02, -3.4173e-02,  1.0359e-01,  5.1044e-01, -1.8806e-01,\n",
       "         -1.7439e-01,  4.4402e-02,  3.3441e-01, -7.4773e-01, -2.3009e-01,\n",
       "         -1.3232e-01, -2.2269e-01,  5.3130e-01,  1.0957e-01,  4.2148e-01,\n",
       "          9.5133e-02, -9.1804e-02, -1.1065e-01, -1.8335e-01,  1.2198e-01,\n",
       "         -1.0728e-01, -2.0161e-01, -9.4322e-02,  1.5743e-01],\n",
       "        [-9.7493e-02,  1.7603e-01,  1.5241e-01, -3.3289e-01, -1.2140e-01,\n",
       "         -1.0607e+00,  7.8047e-03, -2.1590e-01,  2.5986e-02,  1.7098e-01,\n",
       "         -9.6256e-02,  3.6442e-02, -7.2898e-02, -4.3579e-01,  2.6471e-01,\n",
       "         -8.0738e-01,  5.5691e-02,  1.1242e-01, -4.9395e-01, -3.7523e-01,\n",
       "         -1.6213e-01, -2.2354e-01, -9.3263e-02, -2.8991e-02,  2.0438e-01,\n",
       "          2.0574e-01, -4.0414e-02, -7.2572e-02, -2.7682e-01,  5.0825e-02,\n",
       "         -1.2296e-01, -1.7881e-01,  2.3439e-02,  6.9512e-01, -2.8869e-01,\n",
       "         -2.4052e-01, -3.7221e-03,  4.0039e-01, -7.8672e-01, -1.9494e-01,\n",
       "         -1.8045e-01, -1.1162e-01,  6.2499e-01,  1.6384e-01,  4.5163e-01,\n",
       "          6.4067e-02, -1.2222e-01, -1.5075e-01, -2.7694e-01,  7.4218e-02,\n",
       "         -1.6072e-01, -3.7045e-02, -1.9007e-01,  1.0893e-01],\n",
       "        [-7.9598e-02,  2.0706e-01,  1.2190e-01, -3.1251e-01, -1.5515e-01,\n",
       "         -1.1195e+00, -3.7092e-03, -2.6325e-01,  3.3865e-02,  1.3555e-01,\n",
       "         -7.2683e-02, -2.7121e-02, -1.2206e-01, -3.8262e-01,  2.7363e-01,\n",
       "         -8.2655e-01,  5.7115e-02,  1.6934e-01, -4.7380e-01, -3.6267e-01,\n",
       "         -1.6759e-01, -2.0985e-01, -8.6750e-02,  1.8587e-02,  1.9038e-01,\n",
       "          2.8199e-01,  1.0078e-02, -5.7596e-03, -2.8404e-01,  9.8494e-03,\n",
       "         -9.1501e-02, -1.8797e-01,  3.8988e-02,  7.1697e-01, -2.5220e-01,\n",
       "         -2.3338e-01, -3.9481e-02,  3.7716e-01, -8.0064e-01, -1.6480e-01,\n",
       "         -1.6310e-01, -8.0315e-02,  5.8988e-01,  1.6901e-01,  4.2526e-01,\n",
       "          2.7486e-02, -1.3708e-01, -1.2853e-01, -3.3910e-01,  1.1766e-01,\n",
       "         -1.7696e-01, -5.9014e-02, -2.4096e-01,  1.2626e-01],\n",
       "        [-2.9816e-02,  2.6277e-01,  2.7573e-01, -1.9846e-01,  2.2559e-02,\n",
       "         -6.9301e-01, -9.2936e-02, -2.3844e-01, -7.3641e-03,  1.8829e-01,\n",
       "         -3.3602e-02, -6.1115e-02, -1.4666e-01, -4.2134e-01,  3.0623e-01,\n",
       "         -5.1029e-01,  1.3182e-02, -1.5702e-03, -2.7293e-01, -2.7851e-01,\n",
       "         -2.4516e-01, -1.9459e-01, -5.3911e-02,  6.0651e-02,  1.5470e-01,\n",
       "         -7.0344e-02, -3.0006e-04, -1.4057e-01, -2.0467e-01, -1.0908e-01,\n",
       "         -1.4375e-01,  7.5046e-03,  2.1068e-01,  4.6133e-01, -1.2019e-01,\n",
       "         -1.7247e-01,  5.4023e-02,  3.1156e-01, -6.9205e-01, -2.2393e-01,\n",
       "         -6.3608e-02, -2.9281e-01,  5.1273e-01,  6.5679e-02,  3.9605e-01,\n",
       "         -3.0614e-02, -4.5934e-02, -1.3663e-01, -1.0446e-01,  2.3146e-01,\n",
       "         -1.4629e-01, -1.5923e-01, -1.1475e-01,  2.5832e-01],\n",
       "        [-9.5526e-02,  1.6495e-01,  1.1815e-01, -3.4312e-01, -7.6130e-02,\n",
       "         -9.1257e-01,  7.7549e-02, -2.5062e-01,  4.8857e-02,  9.3035e-02,\n",
       "         -1.5724e-01,  2.3541e-02, -7.4555e-02, -4.6082e-01,  3.1616e-01,\n",
       "         -7.7272e-01,  4.0300e-02,  1.3786e-01, -4.4221e-01, -3.9960e-01,\n",
       "         -1.6996e-01, -1.9591e-01, -7.3568e-02,  2.2099e-02,  2.0381e-01,\n",
       "          1.7662e-01, -7.4613e-02, -1.1763e-01, -2.3854e-01, -3.6170e-02,\n",
       "         -1.2957e-01, -1.6758e-01,  4.1475e-02,  6.5088e-01, -3.3313e-01,\n",
       "         -2.6997e-01,  3.3213e-02,  3.8887e-01, -7.5818e-01, -1.6643e-01,\n",
       "         -1.4343e-01, -1.2200e-01,  5.8815e-01,  1.9392e-01,  4.5455e-01,\n",
       "          1.6069e-01, -1.2073e-01, -1.5890e-01, -2.6807e-01,  5.7308e-02,\n",
       "         -1.7135e-01, -1.0256e-01, -1.1548e-01,  1.1222e-01],\n",
       "        [-9.4962e-02,  1.8818e-01,  1.7327e-01, -3.8954e-01, -1.4603e-01,\n",
       "         -1.0523e+00,  6.6462e-02, -1.6402e-01,  4.6901e-02,  1.3539e-01,\n",
       "         -1.6920e-01,  1.9835e-02, -7.5874e-03, -4.0350e-01,  2.6484e-01,\n",
       "         -8.8261e-01,  9.3777e-02,  1.7809e-01, -5.6487e-01, -3.9852e-01,\n",
       "         -1.2345e-01, -1.9190e-01, -4.0353e-02,  4.9148e-03,  1.5741e-01,\n",
       "          1.9260e-01, -1.1640e-01, -1.3902e-01, -2.8494e-01,  1.4582e-02,\n",
       "         -1.5782e-01, -1.8149e-01,  1.4341e-02,  6.7385e-01, -4.2963e-01,\n",
       "         -2.6420e-01,  7.4208e-02,  4.2884e-01, -8.1094e-01, -2.0659e-01,\n",
       "         -2.2047e-01, -1.0576e-01,  6.5975e-01,  2.6149e-01,  4.7469e-01,\n",
       "          1.5959e-01, -1.6958e-01, -1.9907e-01, -3.0768e-01,  6.3333e-02,\n",
       "         -2.1497e-01,  1.4553e-02, -2.4262e-01,  2.2614e-02],\n",
       "        [ 4.3361e-03,  2.3462e-01,  2.3125e-01, -4.5596e-01,  9.1550e-03,\n",
       "         -9.8516e-01,  8.7905e-02, -1.9140e-01, -2.6931e-04,  2.5184e-01,\n",
       "         -1.7391e-01,  3.2681e-02, -1.1453e-02, -4.7414e-01,  2.0497e-01,\n",
       "         -7.9302e-01,  1.1705e-01,  4.6648e-02, -5.2167e-01, -4.5711e-01,\n",
       "         -1.5260e-01, -1.7910e-01, -6.1760e-02, -3.2582e-02,  1.2083e-01,\n",
       "          1.3216e-01, -1.3486e-01, -1.8044e-01, -2.7805e-01,  3.2903e-02,\n",
       "         -1.8788e-01, -1.2371e-01,  9.0604e-03,  5.5080e-01, -4.7677e-01,\n",
       "         -2.8352e-01,  9.6807e-02,  5.1462e-01, -7.5523e-01, -2.1555e-01,\n",
       "         -1.3928e-01, -1.5242e-01,  6.7051e-01,  2.2907e-01,  6.0942e-01,\n",
       "          2.3815e-01, -9.7115e-02, -1.0653e-01, -3.1556e-01, -2.1108e-02,\n",
       "         -1.4144e-01,  6.1250e-02, -1.3923e-01,  1.3041e-01],\n",
       "        [-3.9210e-02,  1.9848e-01,  1.9677e-01, -3.0167e-01, -4.3886e-02,\n",
       "         -7.3666e-01, -2.5710e-02, -2.2825e-01,  1.1192e-02,  1.3202e-01,\n",
       "         -2.9506e-02,  4.2047e-02, -1.1006e-01, -4.5513e-01,  2.7098e-01,\n",
       "         -6.0627e-01,  9.8566e-03,  9.1895e-02, -3.2461e-01, -3.4797e-01,\n",
       "         -1.8563e-01, -2.0056e-01, -5.5475e-02,  6.1006e-02,  1.7923e-01,\n",
       "          7.7768e-02,  1.7737e-02, -1.0626e-01, -2.3119e-01, -6.6625e-02,\n",
       "         -4.7677e-02, -3.4173e-02,  1.0359e-01,  5.1044e-01, -1.8806e-01,\n",
       "         -1.7439e-01,  4.4402e-02,  3.3441e-01, -7.4773e-01, -2.3009e-01,\n",
       "         -1.3232e-01, -2.2269e-01,  5.3130e-01,  1.0957e-01,  4.2148e-01,\n",
       "          9.5133e-02, -9.1804e-02, -1.1065e-01, -1.8335e-01,  1.2198e-01,\n",
       "         -1.0728e-01, -2.0161e-01, -9.4322e-02,  1.5743e-01],\n",
       "        [-7.1204e-02,  2.0681e-01,  2.0283e-01, -3.4527e-01, -2.2835e-02,\n",
       "         -8.1131e-01,  6.1986e-03, -2.1065e-01, -3.9832e-03,  1.5387e-01,\n",
       "         -8.5302e-02,  6.5076e-02, -9.0235e-02, -4.5091e-01,  2.8358e-01,\n",
       "         -6.6954e-01,  3.6606e-02,  7.0507e-02, -4.2530e-01, -3.7940e-01,\n",
       "         -2.2247e-01, -1.7653e-01, -5.1819e-02,  9.9777e-03,  1.5944e-01,\n",
       "          1.2509e-01, -5.0229e-02, -1.6512e-01, -2.8883e-01, -4.3669e-02,\n",
       "         -1.0847e-01, -4.9904e-02,  9.8604e-02,  5.2105e-01, -3.1180e-01,\n",
       "         -2.2943e-01,  7.3667e-02,  4.2169e-01, -7.5437e-01, -1.9932e-01,\n",
       "         -1.0656e-01, -2.2723e-01,  5.7840e-01,  1.4815e-01,  4.9932e-01,\n",
       "          1.3276e-01, -8.6245e-02, -1.3612e-01, -2.1271e-01,  8.3444e-02,\n",
       "         -1.1509e-01, -8.1804e-02, -1.3199e-01,  1.4164e-01],\n",
       "        [-6.6782e-02,  2.2543e-01,  1.3810e-01, -2.6075e-01, -1.7107e-01,\n",
       "         -1.0257e+00, -6.9608e-02, -2.5827e-01,  1.1184e-02,  1.7104e-01,\n",
       "         -1.7747e-02, -6.1051e-02, -1.4360e-01, -3.9444e-01,  2.3242e-01,\n",
       "         -7.2123e-01,  1.4611e-02,  1.1500e-01, -3.8289e-01, -3.0804e-01,\n",
       "         -2.4044e-01, -2.3043e-01, -2.6419e-02,  6.0570e-02,  1.9983e-01,\n",
       "          2.8615e-01,  6.0795e-02,  5.2785e-02, -2.8897e-01, -2.4177e-02,\n",
       "         -1.1999e-01, -1.5972e-01,  1.2711e-01,  7.1137e-01, -1.7106e-01,\n",
       "         -2.1046e-01, -4.9370e-02,  3.8782e-01, -7.5990e-01, -1.5170e-01,\n",
       "         -1.6242e-01, -9.8801e-02,  5.4338e-01,  8.3140e-02,  4.6571e-01,\n",
       "         -8.8918e-02, -1.2439e-01, -9.9129e-02, -2.6440e-01,  1.5127e-01,\n",
       "         -1.7036e-01, -1.5886e-01, -2.8921e-01,  1.8763e-01],\n",
       "        [ 1.4873e-02,  2.8202e-01,  2.8810e-01, -1.6671e-01,  6.3621e-02,\n",
       "         -6.6512e-01, -1.7651e-01, -2.0357e-01, -3.4278e-02,  2.3519e-01,\n",
       "          1.0027e-01, -7.2653e-02, -2.0841e-01, -3.5241e-01,  2.7328e-01,\n",
       "         -4.0439e-01,  3.8376e-02, -7.3021e-02, -1.8394e-01, -2.9532e-01,\n",
       "         -3.5420e-01, -2.2629e-01, -1.9925e-03,  1.0240e-01,  1.0914e-01,\n",
       "         -3.2676e-02,  2.3015e-01, -1.1479e-01, -2.1037e-01, -1.1387e-01,\n",
       "         -6.9164e-02,  1.1825e-01,  2.5784e-01,  3.7891e-01, -1.0996e-02,\n",
       "         -5.8314e-02,  9.1716e-02,  2.1509e-01, -6.8534e-01, -1.9567e-01,\n",
       "         -2.9118e-02, -4.1398e-01,  4.4610e-01,  1.8424e-03,  3.6914e-01,\n",
       "         -5.8010e-02,  3.3792e-02, -3.7986e-02, -5.0935e-02,  2.6166e-01,\n",
       "         -9.2649e-02, -2.0673e-01, -1.8860e-01,  3.1734e-01],\n",
       "        [-1.0145e-01,  1.8791e-01,  1.8181e-01, -3.4157e-01, -7.4308e-02,\n",
       "         -8.8123e-01,  1.3735e-02, -1.9821e-01,  3.1812e-02,  1.1874e-01,\n",
       "         -1.3094e-01, -2.5496e-02, -4.3965e-02, -4.3907e-01,  2.8193e-01,\n",
       "         -7.1738e-01,  7.1977e-02,  1.0998e-01, -4.5959e-01, -3.7469e-01,\n",
       "         -1.8953e-01, -2.1863e-01, -8.8597e-02,  3.1797e-02,  1.8636e-01,\n",
       "          7.2855e-02, -8.8070e-02, -1.9031e-01, -2.6842e-01, -2.3382e-02,\n",
       "         -1.2324e-01, -9.1891e-02,  6.5281e-02,  5.7121e-01, -3.0136e-01,\n",
       "         -2.4197e-01,  9.6522e-02,  4.0954e-01, -7.7477e-01, -2.3958e-01,\n",
       "         -1.6831e-01, -1.8190e-01,  5.8773e-01,  1.6083e-01,  4.6643e-01,\n",
       "          1.1719e-01, -1.2041e-01, -1.8114e-01, -2.4486e-01,  1.3999e-01,\n",
       "         -1.5659e-01, -5.8902e-02, -1.4076e-01,  7.7311e-02],\n",
       "        [-9.5504e-02,  2.1411e-01,  1.0809e-01, -2.8887e-01, -1.6081e-01,\n",
       "         -1.0536e+00, -2.2140e-03, -2.2158e-01,  8.0907e-02,  1.1173e-01,\n",
       "         -1.2771e-01, -2.9685e-02, -1.0570e-01, -4.1567e-01,  2.6717e-01,\n",
       "         -7.8394e-01,  3.6308e-02,  1.6716e-01, -4.5041e-01, -3.3835e-01,\n",
       "         -1.5758e-01, -2.0670e-01, -6.9378e-02,  5.9347e-02,  2.0791e-01,\n",
       "          2.2671e-01, -2.0329e-02, -1.6578e-02, -2.7191e-01, -5.4818e-02,\n",
       "         -1.1487e-01, -1.8301e-01,  6.9707e-02,  6.9763e-01, -2.5484e-01,\n",
       "         -2.5953e-01, -1.0979e-02,  4.1985e-01, -7.7926e-01, -1.6626e-01,\n",
       "         -1.7504e-01, -1.0770e-01,  5.9453e-01,  1.8631e-01,  4.1354e-01,\n",
       "          3.8775e-02, -1.5592e-01, -1.7911e-01, -3.1449e-01,  1.3124e-01,\n",
       "         -2.1609e-01, -7.6384e-02, -2.1559e-01,  1.0022e-01],\n",
       "        [-1.0474e-01,  1.8510e-01,  1.0487e-01, -2.4349e-01, -1.5983e-01,\n",
       "         -9.8870e-01, -2.5614e-02, -2.8974e-01,  4.5953e-02,  1.6078e-01,\n",
       "         -2.5384e-02, -8.1537e-02, -1.3875e-01, -3.9444e-01,  2.6807e-01,\n",
       "         -6.8555e-01,  3.5185e-02,  1.4365e-01, -4.0060e-01, -3.1728e-01,\n",
       "         -1.5692e-01, -2.3357e-01, -1.0982e-01,  1.3813e-01,  2.1525e-01,\n",
       "          2.3131e-01,  3.7383e-02,  3.1287e-02, -2.5008e-01,  1.0084e-03,\n",
       "         -9.6836e-02, -1.9075e-01,  4.3577e-02,  6.9911e-01, -1.7391e-01,\n",
       "         -1.5800e-01, -9.1517e-02,  3.4174e-01, -7.1594e-01, -1.6841e-01,\n",
       "         -1.1950e-01, -1.2888e-01,  5.2095e-01,  1.8231e-01,  4.1822e-01,\n",
       "          5.1604e-03, -1.3193e-01, -7.9789e-02, -2.5802e-01,  1.1281e-01,\n",
       "         -1.2723e-01, -1.6656e-01, -1.5726e-01,  1.2489e-01],\n",
       "        [-6.2364e-02,  2.3027e-01,  2.0757e-01, -2.8260e-01,  1.9508e-02,\n",
       "         -6.7379e-01,  4.5638e-03, -2.5050e-01,  1.9046e-02,  9.9909e-02,\n",
       "         -5.0841e-02,  1.6809e-02, -9.0969e-02, -4.2658e-01,  2.8886e-01,\n",
       "         -5.5474e-01,  6.5763e-04,  4.7837e-02, -3.1962e-01, -3.5168e-01,\n",
       "         -2.2531e-01, -1.9951e-01, -2.3345e-02,  9.2775e-02,  1.6365e-01,\n",
       "         -2.4357e-02,  7.7435e-03, -2.1518e-01, -2.2169e-01, -1.3159e-01,\n",
       "         -7.0054e-02, -1.2850e-02,  1.8685e-01,  4.2753e-01, -1.9590e-01,\n",
       "         -1.6067e-01,  5.3471e-02,  3.4905e-01, -6.8874e-01, -2.4471e-01,\n",
       "         -7.7093e-02, -3.0232e-01,  4.4552e-01,  9.2189e-02,  4.0075e-01,\n",
       "          8.1168e-02, -7.0778e-02, -1.1749e-01, -1.4512e-01,  1.4351e-01,\n",
       "         -1.0964e-01, -1.5779e-01, -9.3285e-02,  1.8184e-01]])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0176, 0.0224, 0.0230, 0.0153, 0.0172, 0.0092, 0.0175, 0.0147, 0.0199,\n",
       "         0.0211, 0.0182, 0.0181, 0.0167, 0.0125, 0.0253, 0.0105, 0.0195, 0.0211,\n",
       "         0.0139, 0.0139, 0.0153, 0.0159, 0.0182, 0.0209, 0.0225, 0.0204, 0.0197,\n",
       "         0.0176, 0.0151, 0.0177, 0.0174, 0.0179, 0.0219, 0.0321, 0.0162, 0.0166,\n",
       "         0.0192, 0.0265, 0.0094, 0.0161, 0.0166, 0.0150, 0.0314, 0.0215, 0.0279,\n",
       "         0.0194, 0.0171, 0.0167, 0.0165, 0.0224, 0.0163, 0.0154, 0.0166, 0.0229],\n",
       "        [0.0182, 0.0231, 0.0231, 0.0140, 0.0181, 0.0091, 0.0185, 0.0151, 0.0192,\n",
       "         0.0216, 0.0184, 0.0198, 0.0170, 0.0120, 0.0248, 0.0103, 0.0191, 0.0208,\n",
       "         0.0137, 0.0134, 0.0157, 0.0155, 0.0179, 0.0201, 0.0227, 0.0205, 0.0193,\n",
       "         0.0170, 0.0150, 0.0177, 0.0181, 0.0183, 0.0210, 0.0316, 0.0157, 0.0159,\n",
       "         0.0198, 0.0265, 0.0090, 0.0151, 0.0166, 0.0152, 0.0322, 0.0211, 0.0289,\n",
       "         0.0208, 0.0173, 0.0170, 0.0158, 0.0214, 0.0170, 0.0155, 0.0172, 0.0222],\n",
       "        [0.0173, 0.0227, 0.0222, 0.0137, 0.0169, 0.0066, 0.0192, 0.0154, 0.0196,\n",
       "         0.0226, 0.0173, 0.0198, 0.0177, 0.0123, 0.0248, 0.0085, 0.0201, 0.0213,\n",
       "         0.0116, 0.0131, 0.0162, 0.0152, 0.0174, 0.0185, 0.0234, 0.0234, 0.0183,\n",
       "         0.0177, 0.0144, 0.0201, 0.0169, 0.0159, 0.0195, 0.0382, 0.0143, 0.0150,\n",
       "         0.0190, 0.0284, 0.0087, 0.0157, 0.0159, 0.0170, 0.0356, 0.0225, 0.0299,\n",
       "         0.0203, 0.0169, 0.0164, 0.0144, 0.0205, 0.0162, 0.0184, 0.0158, 0.0213],\n",
       "        [0.0176, 0.0234, 0.0215, 0.0139, 0.0163, 0.0062, 0.0190, 0.0146, 0.0197,\n",
       "         0.0218, 0.0177, 0.0185, 0.0168, 0.0130, 0.0250, 0.0083, 0.0202, 0.0225,\n",
       "         0.0118, 0.0132, 0.0161, 0.0154, 0.0175, 0.0194, 0.0230, 0.0252, 0.0192,\n",
       "         0.0189, 0.0143, 0.0192, 0.0174, 0.0158, 0.0198, 0.0390, 0.0148, 0.0151,\n",
       "         0.0183, 0.0278, 0.0085, 0.0161, 0.0162, 0.0176, 0.0343, 0.0225, 0.0291,\n",
       "         0.0196, 0.0166, 0.0167, 0.0136, 0.0214, 0.0159, 0.0179, 0.0150, 0.0216],\n",
       "        [0.0183, 0.0246, 0.0249, 0.0155, 0.0193, 0.0094, 0.0172, 0.0149, 0.0187,\n",
       "         0.0228, 0.0183, 0.0178, 0.0163, 0.0124, 0.0257, 0.0113, 0.0191, 0.0189,\n",
       "         0.0144, 0.0143, 0.0148, 0.0155, 0.0179, 0.0201, 0.0220, 0.0176, 0.0189,\n",
       "         0.0164, 0.0154, 0.0169, 0.0164, 0.0190, 0.0233, 0.0300, 0.0167, 0.0159,\n",
       "         0.0199, 0.0258, 0.0095, 0.0151, 0.0177, 0.0141, 0.0315, 0.0202, 0.0281,\n",
       "         0.0183, 0.0180, 0.0165, 0.0170, 0.0238, 0.0163, 0.0161, 0.0168, 0.0245],\n",
       "        [0.0173, 0.0225, 0.0215, 0.0135, 0.0177, 0.0077, 0.0206, 0.0148, 0.0200,\n",
       "         0.0209, 0.0163, 0.0195, 0.0177, 0.0120, 0.0262, 0.0088, 0.0198, 0.0219,\n",
       "         0.0122, 0.0128, 0.0161, 0.0157, 0.0177, 0.0195, 0.0234, 0.0227, 0.0177,\n",
       "         0.0169, 0.0150, 0.0184, 0.0167, 0.0161, 0.0199, 0.0365, 0.0137, 0.0146,\n",
       "         0.0197, 0.0281, 0.0089, 0.0161, 0.0165, 0.0169, 0.0343, 0.0231, 0.0300,\n",
       "         0.0224, 0.0169, 0.0163, 0.0146, 0.0202, 0.0161, 0.0172, 0.0170, 0.0213],\n",
       "        [0.0173, 0.0230, 0.0227, 0.0129, 0.0165, 0.0067, 0.0204, 0.0162, 0.0200,\n",
       "         0.0218, 0.0161, 0.0195, 0.0189, 0.0127, 0.0249, 0.0079, 0.0209, 0.0228,\n",
       "         0.0108, 0.0128, 0.0169, 0.0157, 0.0183, 0.0192, 0.0223, 0.0231, 0.0170,\n",
       "         0.0166, 0.0143, 0.0194, 0.0163, 0.0159, 0.0193, 0.0374, 0.0124, 0.0146,\n",
       "         0.0205, 0.0293, 0.0085, 0.0155, 0.0153, 0.0172, 0.0369, 0.0248, 0.0307,\n",
       "         0.0224, 0.0161, 0.0156, 0.0140, 0.0203, 0.0154, 0.0194, 0.0150, 0.0195],\n",
       "        [0.0189, 0.0238, 0.0237, 0.0119, 0.0190, 0.0070, 0.0206, 0.0156, 0.0188,\n",
       "         0.0242, 0.0158, 0.0195, 0.0186, 0.0117, 0.0231, 0.0085, 0.0212, 0.0197,\n",
       "         0.0112, 0.0119, 0.0162, 0.0158, 0.0177, 0.0182, 0.0213, 0.0215, 0.0165,\n",
       "         0.0157, 0.0143, 0.0195, 0.0156, 0.0167, 0.0190, 0.0327, 0.0117, 0.0142,\n",
       "         0.0208, 0.0315, 0.0089, 0.0152, 0.0164, 0.0162, 0.0368, 0.0237, 0.0347,\n",
       "         0.0239, 0.0171, 0.0169, 0.0137, 0.0184, 0.0164, 0.0200, 0.0164, 0.0215],\n",
       "        [0.0182, 0.0231, 0.0231, 0.0140, 0.0181, 0.0091, 0.0185, 0.0151, 0.0192,\n",
       "         0.0216, 0.0184, 0.0198, 0.0170, 0.0120, 0.0248, 0.0103, 0.0191, 0.0208,\n",
       "         0.0137, 0.0134, 0.0157, 0.0155, 0.0179, 0.0201, 0.0227, 0.0205, 0.0193,\n",
       "         0.0170, 0.0150, 0.0177, 0.0181, 0.0183, 0.0210, 0.0316, 0.0157, 0.0159,\n",
       "         0.0198, 0.0265, 0.0090, 0.0151, 0.0166, 0.0152, 0.0322, 0.0211, 0.0289,\n",
       "         0.0208, 0.0173, 0.0170, 0.0158, 0.0214, 0.0170, 0.0155, 0.0172, 0.0222],\n",
       "        [0.0177, 0.0233, 0.0232, 0.0134, 0.0185, 0.0084, 0.0191, 0.0154, 0.0189,\n",
       "         0.0221, 0.0174, 0.0202, 0.0173, 0.0121, 0.0252, 0.0097, 0.0197, 0.0203,\n",
       "         0.0124, 0.0130, 0.0152, 0.0159, 0.0180, 0.0192, 0.0222, 0.0215, 0.0180,\n",
       "         0.0161, 0.0142, 0.0182, 0.0170, 0.0180, 0.0209, 0.0319, 0.0139, 0.0151,\n",
       "         0.0204, 0.0289, 0.0089, 0.0155, 0.0170, 0.0151, 0.0338, 0.0220, 0.0312,\n",
       "         0.0217, 0.0174, 0.0165, 0.0153, 0.0206, 0.0169, 0.0175, 0.0166, 0.0218],\n",
       "        [0.0177, 0.0238, 0.0218, 0.0146, 0.0160, 0.0068, 0.0177, 0.0146, 0.0192,\n",
       "         0.0225, 0.0186, 0.0178, 0.0164, 0.0128, 0.0239, 0.0092, 0.0192, 0.0213,\n",
       "         0.0129, 0.0139, 0.0149, 0.0151, 0.0185, 0.0201, 0.0232, 0.0252, 0.0201,\n",
       "         0.0200, 0.0142, 0.0185, 0.0168, 0.0162, 0.0215, 0.0386, 0.0160, 0.0154,\n",
       "         0.0180, 0.0279, 0.0089, 0.0163, 0.0161, 0.0172, 0.0326, 0.0206, 0.0302,\n",
       "         0.0173, 0.0167, 0.0172, 0.0146, 0.0221, 0.0160, 0.0162, 0.0142, 0.0229],\n",
       "        [0.0189, 0.0247, 0.0248, 0.0157, 0.0198, 0.0096, 0.0156, 0.0152, 0.0180,\n",
       "         0.0235, 0.0206, 0.0173, 0.0151, 0.0131, 0.0244, 0.0124, 0.0193, 0.0173,\n",
       "         0.0155, 0.0138, 0.0131, 0.0148, 0.0186, 0.0206, 0.0207, 0.0180, 0.0234,\n",
       "         0.0166, 0.0151, 0.0166, 0.0174, 0.0209, 0.0241, 0.0272, 0.0184, 0.0175,\n",
       "         0.0204, 0.0231, 0.0094, 0.0153, 0.0181, 0.0123, 0.0291, 0.0186, 0.0269,\n",
       "         0.0175, 0.0192, 0.0179, 0.0177, 0.0242, 0.0170, 0.0151, 0.0154, 0.0255],\n",
       "        [0.0173, 0.0231, 0.0229, 0.0136, 0.0178, 0.0079, 0.0194, 0.0157, 0.0198,\n",
       "         0.0215, 0.0168, 0.0187, 0.0183, 0.0123, 0.0254, 0.0093, 0.0206, 0.0214,\n",
       "         0.0121, 0.0132, 0.0158, 0.0154, 0.0175, 0.0198, 0.0231, 0.0206, 0.0175,\n",
       "         0.0158, 0.0146, 0.0187, 0.0169, 0.0175, 0.0204, 0.0339, 0.0142, 0.0150,\n",
       "         0.0211, 0.0288, 0.0088, 0.0151, 0.0162, 0.0160, 0.0344, 0.0225, 0.0305,\n",
       "         0.0215, 0.0170, 0.0160, 0.0150, 0.0220, 0.0164, 0.0180, 0.0166, 0.0207],\n",
       "        [0.0173, 0.0236, 0.0212, 0.0143, 0.0162, 0.0066, 0.0190, 0.0153, 0.0207,\n",
       "         0.0213, 0.0168, 0.0185, 0.0172, 0.0126, 0.0249, 0.0087, 0.0198, 0.0225,\n",
       "         0.0122, 0.0136, 0.0163, 0.0155, 0.0178, 0.0202, 0.0235, 0.0239, 0.0187,\n",
       "         0.0188, 0.0145, 0.0180, 0.0170, 0.0159, 0.0204, 0.0383, 0.0148, 0.0147,\n",
       "         0.0189, 0.0290, 0.0087, 0.0161, 0.0160, 0.0171, 0.0345, 0.0230, 0.0288,\n",
       "         0.0198, 0.0163, 0.0159, 0.0139, 0.0217, 0.0154, 0.0177, 0.0154, 0.0211],\n",
       "        [0.0171, 0.0228, 0.0210, 0.0149, 0.0162, 0.0071, 0.0185, 0.0142, 0.0198,\n",
       "         0.0223, 0.0185, 0.0175, 0.0165, 0.0128, 0.0248, 0.0095, 0.0196, 0.0219,\n",
       "         0.0127, 0.0138, 0.0162, 0.0150, 0.0170, 0.0218, 0.0235, 0.0239, 0.0197,\n",
       "         0.0196, 0.0148, 0.0190, 0.0172, 0.0157, 0.0198, 0.0381, 0.0159, 0.0162,\n",
       "         0.0173, 0.0267, 0.0093, 0.0160, 0.0168, 0.0167, 0.0319, 0.0227, 0.0288,\n",
       "         0.0190, 0.0166, 0.0175, 0.0146, 0.0212, 0.0167, 0.0160, 0.0162, 0.0215],\n",
       "        [0.0179, 0.0239, 0.0234, 0.0143, 0.0194, 0.0097, 0.0191, 0.0148, 0.0194,\n",
       "         0.0210, 0.0181, 0.0193, 0.0174, 0.0124, 0.0254, 0.0109, 0.0190, 0.0200,\n",
       "         0.0138, 0.0134, 0.0152, 0.0156, 0.0186, 0.0209, 0.0224, 0.0186, 0.0192,\n",
       "         0.0153, 0.0152, 0.0167, 0.0177, 0.0188, 0.0229, 0.0292, 0.0156, 0.0162,\n",
       "         0.0201, 0.0270, 0.0096, 0.0149, 0.0176, 0.0141, 0.0297, 0.0209, 0.0284,\n",
       "         0.0206, 0.0177, 0.0169, 0.0165, 0.0220, 0.0170, 0.0162, 0.0173, 0.0228]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = nn.functional.softmax(logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33, 42, 33, 33, 42, 33, 33, 42, 42, 42, 33, 42, 42, 33, 33, 42])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = probs.argmax(dim=1)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([65, 78, 64, 75, 71, 56, 70, 80, 78, 48, 74, 57, 77, 63, 57, 72])"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77, 86, 77, 77, 86, 77, 77, 86, 86, 86, 77, 86, 86, 77, 77, 86]"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_dataset.id2label[idx.item()] for idx in idxs]# prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze all except 2 last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.requires_grad_ = False\n",
    "\n",
    "model.bert.pooler.requires_grad_ = True\n",
    "model.classifier.requires_grad_ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_epochs = 25\n",
    "learning_rate = 1e-6\n",
    "hidden_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningBert(pl.LightningModule):\n",
    "    def __init__(self, hparams=None, model=None, train_id2label=None, val_id2label=None):\n",
    "        super(LightningBert, self).__init__()\n",
    "        self.model = model\n",
    "        self.metric = f1_score\n",
    "        self.criterion == nn.CrossEntropyLoss()\n",
    "        self.train_id2label = train_id2label\n",
    "        self.val_id2label = val_id2label\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.model(\n",
    "            x['input_ids'].unsqueeze(0),\n",
    "            x['attention_mask'].unsqueeze(0),\n",
    "        ).logits\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_masks = batch['attention_masks']\n",
    "        labels = batch['targets']\n",
    "        \n",
    "        logits = self.forward(input_ids, attention_masks)\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        idxs = probs.argmax(dim=1)\n",
    "        preds = [self.train_id2label[idx.item()] for idx in idxs]\n",
    "        loss = self.criterion(logits, labels)\n",
    "        metric = self.metric(preds, labels)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        self.log(\"train_f1\", metric, on_epoch=True)\n",
    "        \n",
    "        return {'loss' : loss, 'train_acc' : metric}\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_masks = batch['attention_masks']\n",
    "        labels = batch['targets']\n",
    "\n",
    "        logits = self.forward(input_ids, attention_masks)\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        idxs = probs.argmax(dim=1)\n",
    "        preds = [self.val_id2label[idx.item()] for idx in idxs]\n",
    "        loss = self.criterion(logits, labels)\n",
    "        metric = self.metric(preds, labels)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        self.log(\"val_f1\", metric, on_epoch=True)\n",
    "        \n",
    "        return {'loss' : loss, 'train_acc' : metric}\n",
    "    \n",
    "    def predict_step(self, batch):\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_masks = batch['attention_masks']\n",
    "\n",
    "        logits = self.model(input_ids, attention_masks).logits\n",
    "        probs = nn.functional.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        idxs = probs.argmax(dim=1)\n",
    "        preds = [self.val_id2label[idx.item()] for idx in idxs]\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-5)\n",
    "        plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                                    mode='min',\n",
    "                                                                    factor=0.5,\n",
    "                                                                    patience=5,\n",
    "                                                                    verbose=True)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': plateau_scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
